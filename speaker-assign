#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "pyyaml>=6.0",
# ]
# ///
"""
speaker-assign - Multi-signal speaker name assignment

Combine embedding matches, LLM analysis, and context hints to assign
speaker names to transcript labels (S1, S2, A, B, etc.).

Usage:
    speaker-assign <audio> --transcript FILE [OPTIONS]
    speaker-assign show <audio>
    speaker-assign clear <audio>
"""

from __future__ import annotations

import argparse
import json
import os
import subprocess
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Optional

# Optional YAML support
try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False


# =============================================================================
# Constants and Configuration
# =============================================================================

VERSION = "1.0.0"
SCHEMA_VERSION = 1

# Signal weights for combining scores
SIGNAL_WEIGHTS = {
    "embedding_match": 0.4,
    "llm_name_detection": 0.3,
    "context_expected": 0.2,
    "cross_backend_agreement": 0.1,
}

# Trust level multipliers for embedding matches
TRUST_MULTIPLIERS = {
    "high": 1.0,
    "medium": 0.7,
    "low": 0.4,
    "invalidated": 0.0,
    "unknown": 0.5,
}

# Confidence thresholds
CONFIDENCE_THRESHOLDS = {
    "high": 0.7,
    "medium": 0.4,
    "low": 0.2,
}


def get_speakers_embeddings_dir() -> Path:
    """Get the root directory for speaker embeddings storage."""
    return Path(os.environ.get(
        "SPEAKERS_EMBEDDINGS_DIR",
        os.path.expanduser("~/.config/speakers_embeddings")
    ))


def get_assignments_dir() -> Path:
    """Get the assignments directory."""
    assignments_dir = get_speakers_embeddings_dir() / "assignments"
    assignments_dir.mkdir(parents=True, exist_ok=True)
    return assignments_dir


def get_catalog_dir() -> Path:
    """Get the catalog directory."""
    return get_speakers_embeddings_dir() / "catalog"


def get_db_dir() -> Path:
    """Get the speaker profiles directory."""
    return get_speakers_embeddings_dir() / "db"


# =============================================================================
# Utilities
# =============================================================================

def compute_b3sum(file_path: Path) -> str:
    """Compute Blake3 hash of a file, falling back to SHA256 if b3sum unavailable."""
    try:
        result = subprocess.run(
            ["b3sum", "--no-names", str(file_path)],
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()[:32]
    except (subprocess.CalledProcessError, FileNotFoundError):
        import hashlib
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                sha256.update(chunk)
        return sha256.hexdigest()[:32]


def utc_now_iso() -> str:
    """Return current UTC time in ISO format."""
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def load_yaml(path: Path) -> dict:
    """Load YAML file, with JSON fallback."""
    with open(path, "r") as f:
        content = f.read()
    if YAML_AVAILABLE:
        return yaml.safe_load(content) or {}
    else:
        return json.loads(content)


def save_yaml(path: Path, data: dict) -> None:
    """Save data as YAML, with JSON fallback."""
    with open(path, "w") as f:
        if YAML_AVAILABLE:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False, allow_unicode=True)
        else:
            json.dump(data, f, indent=2, ensure_ascii=False)


def resolve_audio_b3sum(audio_arg: str) -> Optional[str]:
    """Resolve audio argument to b3sum."""
    # Check if it's already a b3sum prefix
    if len(audio_arg) >= 6 and len(audio_arg) <= 32 and all(c in "0123456789abcdef" for c in audio_arg.lower()):
        catalog_dir = get_catalog_dir()
        matches = list(catalog_dir.glob(f"{audio_arg.lower()}*.yaml"))
        if len(matches) == 1:
            return matches[0].stem
        elif len(matches) > 1:
            print(f"Error: Ambiguous b3sum prefix '{audio_arg}'", file=sys.stderr)
            return None

    # Treat as file path
    audio_path = Path(audio_arg).resolve()
    if audio_path.exists():
        return compute_b3sum(audio_path)

    return None


# =============================================================================
# Transcript Parsing
# =============================================================================

def detect_transcript_format(data: dict) -> str:
    """Detect transcript format: speechmatics, assemblyai, or unknown."""
    if "utterances" in data:
        return "assemblyai"
    if "results" in data:
        return "speechmatics"
    return "unknown"


def get_speakers_from_transcript(data: dict) -> list[str]:
    """Extract unique speaker labels from transcript."""
    fmt = detect_transcript_format(data)
    speakers = set()

    if fmt == "assemblyai":
        for u in data.get("utterances", []):
            if u.get("speaker"):
                speakers.add(u["speaker"])
    elif fmt == "speechmatics":
        for r in data.get("results", []):
            for alt in r.get("alternatives", []):
                if alt.get("speaker"):
                    speakers.add(alt["speaker"])
            # Also check top-level speaker field
            if r.get("speaker"):
                speakers.add(r["speaker"])

    return sorted(speakers)


def get_speaker_segments(data: dict, speaker_label: str) -> list[dict]:
    """Get segments for a specific speaker."""
    fmt = detect_transcript_format(data)
    segments = []

    if fmt == "assemblyai":
        for u in data.get("utterances", []):
            if u.get("speaker") == speaker_label:
                segments.append({
                    "start": u.get("start", 0) / 1000.0,  # Convert ms to seconds
                    "end": u.get("end", 0) / 1000.0,
                    "text": u.get("text", ""),
                })
    elif fmt == "speechmatics":
        # Group consecutive words by speaker
        current_segment = None
        for r in data.get("results", []):
            speaker = None
            text = ""
            for alt in r.get("alternatives", []):
                if alt.get("speaker"):
                    speaker = alt["speaker"]
                if alt.get("content"):
                    text = alt["content"]

            if r.get("speaker"):
                speaker = r["speaker"]

            if speaker == speaker_label and r.get("start_time") is not None:
                if current_segment is None:
                    current_segment = {
                        "start": r["start_time"],
                        "end": r.get("end_time", r["start_time"]),
                        "text": text,
                    }
                else:
                    # Extend current segment
                    current_segment["end"] = r.get("end_time", r["start_time"])
                    if text:
                        current_segment["text"] += " " + text
            elif current_segment is not None:
                segments.append(current_segment)
                current_segment = None

        if current_segment is not None:
            segments.append(current_segment)

    return segments


# =============================================================================
# Signal Collection
# =============================================================================

@dataclass
class Signal:
    """A signal contributing to speaker assignment."""
    type: str
    speaker_id: Optional[str]
    score: float
    evidence: dict = field(default_factory=dict)


def collect_embedding_signals(
    speaker_label: str,
    segments: list[dict],
    audio_path: Path,
    min_trust: str = "low",
    tags: Optional[list[str]] = None,
) -> list[Signal]:
    """
    Collect embedding match signals by calling speaker_detection identify.

    Returns signals with embedding match scores.
    """
    signals = []

    # We need actual audio segments to identify
    # For now, we'll call speaker_detection identify on the whole audio or segments
    # This is a simplified implementation - real version would extract segments

    # Check if speaker_detection is available
    try:
        # Build identify command
        cmd = ["speaker_detection", "identify", str(audio_path)]
        if tags:
            cmd.extend(["--tags", ",".join(tags)])
        cmd.append("--format")
        cmd.append("json")

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            env=os.environ.copy(),
        )

        if result.returncode == 0 and result.stdout.strip():
            try:
                matches = json.loads(result.stdout)
                if isinstance(matches, list):
                    for match in matches:
                        if match.get("speaker_id"):
                            trust = match.get("trust_level", "unknown")

                            # Skip if trust level is below minimum
                            trust_order = ["low", "medium", "high"]
                            if min_trust in trust_order:
                                min_idx = trust_order.index(min_trust)
                                if trust in trust_order:
                                    trust_idx = trust_order.index(trust)
                                    if trust_idx < min_idx:
                                        continue

                            signals.append(Signal(
                                type="embedding_match",
                                speaker_id=match["speaker_id"],
                                score=match.get("score", 0.5),
                                evidence={
                                    "embedding_id": match.get("embedding_id"),
                                    "trust_level": trust,
                                    "backend": match.get("backend"),
                                }
                            ))
            except json.JSONDecodeError:
                pass
    except FileNotFoundError:
        pass

    return signals


def collect_context_signals(
    speaker_label: str,
    context_name: Optional[str],
    expected_speakers: list[str],
) -> list[Signal]:
    """
    Collect context-based signals from expected speakers list.
    """
    signals = []

    # If we have expected speakers, create signals for each
    for speaker_id in expected_speakers:
        signals.append(Signal(
            type="context_expected",
            speaker_id=speaker_id,
            score=0.5,  # Base score for being expected
            evidence={
                "context": context_name,
                "reason": "in expected_speakers list",
            }
        ))

    return signals


def collect_llm_signals(
    speaker_label: str,
    transcript_path: Path,
    context_name: Optional[str] = None,
) -> list[Signal]:
    """
    Collect LLM-based name detection signals.

    Calls speaker-llm if available.
    """
    signals = []

    # Check if speaker-llm is available
    try:
        cmd = ["speaker-llm", "analyze", str(transcript_path)]
        if context_name:
            cmd.extend(["--context", context_name])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            env=os.environ.copy(),
        )

        if result.returncode == 0 and result.stdout.strip():
            try:
                analysis = json.loads(result.stdout)
                for detection in analysis.get("detections", []):
                    if detection.get("speaker_label") == speaker_label:
                        signals.append(Signal(
                            type="llm_name_detection",
                            speaker_id=detection.get("detected_name", "").lower().replace(" ", "-"),
                            score=detection.get("confidence", 0.5),
                            evidence={
                                "detected_name": detection.get("detected_name"),
                                "evidence": detection.get("evidence", []),
                            }
                        ))
            except json.JSONDecodeError:
                pass
    except FileNotFoundError:
        pass

    return signals


# =============================================================================
# Signal Combination
# =============================================================================

@dataclass
class Assignment:
    """Speaker assignment result."""
    speaker_label: str
    speaker_id: Optional[str]
    confidence: str  # confirmed, high, medium, low, unassigned
    score: float
    signals: list[dict]
    candidates: list[dict] = field(default_factory=list)


def combine_signals(
    speaker_label: str,
    signals: list[Signal],
    threshold: float = 0.5,
) -> Assignment:
    """
    Combine signals to produce a speaker assignment.

    Uses weighted scoring with trust multipliers for embeddings.
    """
    scores: dict[str, float] = defaultdict(float)
    evidence: dict[str, list] = defaultdict(list)

    for signal in signals:
        if signal.speaker_id is None:
            continue

        weight = SIGNAL_WEIGHTS.get(signal.type, 0.1)

        # Apply trust multiplier for embedding matches
        if signal.type == "embedding_match":
            trust = signal.evidence.get("trust_level", "unknown")
            weight *= TRUST_MULTIPLIERS.get(trust, 0.5)

        weighted_score = weight * signal.score
        scores[signal.speaker_id] += weighted_score
        evidence[signal.speaker_id].append({
            "type": signal.type,
            "score": signal.score,
            **signal.evidence,
        })

    if not scores:
        return Assignment(
            speaker_label=speaker_label,
            speaker_id=None,
            confidence="unassigned",
            score=0.0,
            signals=[],
            candidates=[],
        )

    # Sort by score
    sorted_candidates = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    best_id, best_score = sorted_candidates[0]

    # Determine confidence level
    if best_score >= CONFIDENCE_THRESHOLDS["high"]:
        confidence = "high"
    elif best_score >= CONFIDENCE_THRESHOLDS["medium"]:
        confidence = "medium"
    elif best_score >= CONFIDENCE_THRESHOLDS["low"]:
        confidence = "low"
    else:
        confidence = "unassigned"

    # If below threshold, mark as unassigned
    if best_score < threshold:
        return Assignment(
            speaker_label=speaker_label,
            speaker_id=None,
            confidence="unassigned",
            score=best_score,
            signals=evidence.get(best_id, []),
            candidates=[{"speaker_id": sid, "score": sc} for sid, sc in sorted_candidates[:3]],
        )

    return Assignment(
        speaker_label=speaker_label,
        speaker_id=best_id,
        confidence=confidence,
        score=best_score,
        signals=evidence.get(best_id, []),
        candidates=[{"speaker_id": sid, "score": sc} for sid, sc in sorted_candidates[1:4]],
    )


# =============================================================================
# Commands
# =============================================================================

def cmd_assign(args: argparse.Namespace) -> int:
    """Assign speaker names to transcript labels."""
    audio_path = Path(args.audio).resolve()
    transcript_path = Path(args.transcript).resolve()

    if not audio_path.exists():
        print(f"Error: Audio file not found: {audio_path}", file=sys.stderr)
        return 1

    if not transcript_path.exists():
        print(f"Error: Transcript file not found: {transcript_path}", file=sys.stderr)
        return 1

    # Load transcript
    with open(transcript_path, "r") as f:
        transcript_data = json.load(f)

    # Get speaker labels from transcript
    speaker_labels = get_speakers_from_transcript(transcript_data)

    if not speaker_labels:
        print("Error: No speakers found in transcript", file=sys.stderr)
        return 1

    if not args.quiet:
        print(f"Found {len(speaker_labels)} speakers: {', '.join(speaker_labels)}")

    # Get context information
    context_name = args.context
    expected_speakers = []

    # Try to get context from catalog
    b3sum = compute_b3sum(audio_path)
    catalog_path = get_catalog_dir() / f"{b3sum}.yaml"
    if catalog_path.exists():
        catalog_entry = load_yaml(catalog_path)
        if not context_name:
            context_name = catalog_entry.get("context", {}).get("name")
        if not expected_speakers:
            expected_speakers = catalog_entry.get("context", {}).get("expected_speakers", [])

    if args.expected_speakers:
        expected_speakers = args.expected_speakers.split(",")

    # Process each speaker label
    assignments = {}
    for label in speaker_labels:
        segments = get_speaker_segments(transcript_data, label)

        if args.verbose:
            print(f"\nProcessing speaker {label} ({len(segments)} segments)...")

        all_signals = []

        # Collect embedding signals
        if args.use_embeddings:
            if args.verbose:
                print(f"  Collecting embedding signals...")
            embedding_signals = collect_embedding_signals(
                label,
                segments,
                audio_path,
                min_trust=args.min_trust,
                tags=args.tags.split(",") if args.tags else None,
            )
            all_signals.extend(embedding_signals)
            if args.verbose and embedding_signals:
                for s in embedding_signals:
                    print(f"    - {s.speaker_id}: {s.score:.2f} (trust: {s.evidence.get('trust_level', '-')})")

        # Collect context signals
        if expected_speakers:
            if args.verbose:
                print(f"  Collecting context signals...")
            context_signals = collect_context_signals(label, context_name, expected_speakers)
            all_signals.extend(context_signals)

        # Collect LLM signals
        if args.use_llm:
            if args.verbose:
                print(f"  Collecting LLM signals...")
            llm_signals = collect_llm_signals(label, transcript_path, context_name)
            all_signals.extend(llm_signals)
            if args.verbose and llm_signals:
                for s in llm_signals:
                    print(f"    - {s.speaker_id}: {s.score:.2f}")

        # Combine signals
        assignment = combine_signals(label, all_signals, threshold=args.threshold)
        assignments[label] = {
            "speaker_id": assignment.speaker_id,
            "confidence": assignment.confidence,
            "score": round(assignment.score, 3),
            "signals": assignment.signals,
        }
        if assignment.candidates:
            assignments[label]["candidates"] = assignment.candidates

    # Build output
    output = {
        "schema_version": SCHEMA_VERSION,
        "recording_b3sum": b3sum,
        "transcript_path": str(transcript_path),
        "assigned_at": utc_now_iso(),
        "method": f"speaker-assign-v{VERSION}",
        "context": context_name,
        "min_trust": args.min_trust,
        "threshold": args.threshold,
        "mappings": assignments,
    }

    # Output
    if args.dry_run:
        print("\n=== DRY RUN - No changes saved ===")
        if args.format == "json":
            print(json.dumps(output, indent=2, ensure_ascii=False))
        else:
            print(f"\nAssignments for: {audio_path.name}")
            print("-" * 50)
            for label, data in assignments.items():
                speaker = data.get("speaker_id") or "(unassigned)"
                conf = data.get("confidence", "?")
                score = data.get("score", 0)
                print(f"  {label} -> {speaker} ({conf}, score: {score:.2f})")
                if data.get("candidates"):
                    print(f"       candidates: {', '.join(c['speaker_id'] for c in data['candidates'])}")
        return 0

    # Save assignments
    assignments_path = get_assignments_dir() / f"{b3sum}.yaml"
    save_yaml(assignments_path, output)

    # Also save to output file if specified
    if args.output:
        output_path = Path(args.output)
        save_yaml(output_path, output)

    if args.format == "json":
        print(json.dumps(output, indent=2, ensure_ascii=False))
    elif not args.quiet:
        print(f"\nAssignments saved: {assignments_path.name}")
        print("-" * 50)
        assigned_count = sum(1 for a in assignments.values() if a.get("speaker_id"))
        print(f"Assigned: {assigned_count}/{len(assignments)}")
        for label, data in assignments.items():
            speaker = data.get("speaker_id") or "(unassigned)"
            conf = data.get("confidence", "?")
            score = data.get("score", 0)
            print(f"  {label} -> {speaker} ({conf}, score: {score:.2f})")

    return 0


def cmd_show(args: argparse.Namespace) -> int:
    """Show current assignments for a recording."""
    b3sum = resolve_audio_b3sum(args.audio)
    if not b3sum:
        print(f"Error: Could not resolve audio: {args.audio}", file=sys.stderr)
        return 1

    assignments_path = get_assignments_dir() / f"{b3sum}.yaml"
    if not assignments_path.exists():
        print(f"Error: No assignments found for this recording", file=sys.stderr)
        return 1

    data = load_yaml(assignments_path)

    if args.format == "json":
        print(json.dumps(data, indent=2, ensure_ascii=False))
    elif args.format == "yaml":
        if YAML_AVAILABLE:
            print(yaml.dump(data, default_flow_style=False, sort_keys=False, allow_unicode=True))
        else:
            print(json.dumps(data, indent=2, ensure_ascii=False))
    else:
        # Human-readable format
        print(f"Assignments for: {b3sum[:8]}...")
        print(f"Context: {data.get('context') or '-'}")
        print(f"Method: {data.get('method', '-')}")
        print(f"Assigned at: {data.get('assigned_at', '-')}")
        print(f"Threshold: {data.get('threshold', '-')}")
        print(f"Min trust: {data.get('min_trust', '-')}")
        print()

        mappings = data.get("mappings", {})
        if mappings:
            print("Mappings:")
            for label, info in mappings.items():
                speaker = info.get("speaker_id") or "(unassigned)"
                conf = info.get("confidence", "?")
                score = info.get("score", 0)
                print(f"  {label} -> {speaker}")
                print(f"       confidence: {conf}, score: {score:.3f}")
                if info.get("signals"):
                    print(f"       signals: {len(info['signals'])}")
                    for sig in info["signals"][:3]:
                        print(f"         - {sig.get('type', '?')}: {sig.get('score', 0):.2f}")
                if info.get("candidates"):
                    cands = ", ".join(f"{c['speaker_id']}({c['score']:.2f})" for c in info["candidates"])
                    print(f"       candidates: {cands}")
        else:
            print("No mappings found")

    return 0


def cmd_clear(args: argparse.Namespace) -> int:
    """Clear assignments for a recording."""
    b3sum = resolve_audio_b3sum(args.audio)
    if not b3sum:
        print(f"Error: Could not resolve audio: {args.audio}", file=sys.stderr)
        return 1

    assignments_path = get_assignments_dir() / f"{b3sum}.yaml"
    if not assignments_path.exists():
        print(f"No assignments found for this recording", file=sys.stderr)
        return 0

    if not args.force:
        print(f"Clear assignments for: {b3sum[:8]}...?")
        response = input("Confirm [y/N]: ")
        if response.lower() != "y":
            print("Cancelled")
            return 0

    assignments_path.unlink()
    if not args.quiet:
        print(f"Cleared assignments: {b3sum[:8]}...")

    return 0


# =============================================================================
# Main
# =============================================================================

def main() -> int:
    parser = argparse.ArgumentParser(
        description="Multi-signal speaker name assignment",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("-V", "--version", action="version", version=f"speaker-assign {VERSION}")
    parser.add_argument("-q", "--quiet", action="store_true", help="Suppress non-essential output")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")

    subparsers = parser.add_subparsers(dest="command")

    # assign command (default when audio and transcript provided)
    assign_parser = subparsers.add_parser("assign", help="Assign speaker names to transcript labels")
    assign_parser.add_argument("audio", help="Path to audio file")
    assign_parser.add_argument("--transcript", "-t", required=True, help="Path to transcript JSON file")
    assign_parser.add_argument("--use-embeddings", "-e", action="store_true", help="Use speaker_detection identify")
    assign_parser.add_argument("--min-trust", default="low", choices=["high", "medium", "low"], help="Minimum embedding trust level")
    assign_parser.add_argument("--use-llm", "-l", action="store_true", help="Use LLM conversation analysis")
    assign_parser.add_argument("--context", "-c", help="Speaker context for name resolution")
    assign_parser.add_argument("--expected-speakers", help="Comma-separated expected speaker IDs")
    assign_parser.add_argument("--tags", help="Filter speakers by tags (for embedding lookup)")
    assign_parser.add_argument("--threshold", type=float, default=0.3, help="Minimum confidence for assignment")
    assign_parser.add_argument("--output", "-o", help="Output file for assignment results")
    assign_parser.add_argument("--format", "-f", choices=["text", "json"], default="text")
    assign_parser.add_argument("--dry-run", "-n", action="store_true", help="Show assignments without saving")
    assign_parser.set_defaults(func=cmd_assign)

    # show command
    show_parser = subparsers.add_parser("show", help="Show current assignments for a recording")
    show_parser.add_argument("audio", help="Path to audio file or b3sum prefix")
    show_parser.add_argument("--format", "-f", choices=["text", "json", "yaml"], default="text")
    show_parser.set_defaults(func=cmd_show)

    # clear command
    clear_parser = subparsers.add_parser("clear", help="Clear assignments for a recording")
    clear_parser.add_argument("audio", help="Path to audio file or b3sum prefix")
    clear_parser.add_argument("--force", "-f", action="store_true", help="Skip confirmation")
    clear_parser.set_defaults(func=cmd_clear)

    args = parser.parse_args()

    # If no subcommand but audio provided, assume 'assign'
    if not args.command:
        parser.print_help()
        return 0

    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
