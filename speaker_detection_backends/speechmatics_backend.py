"""
Speechmatics speaker embedding backend.

Uses Speechmatics Batch API for speaker enrollment and identification.
Speaker identifiers are encrypted strings generated by Speechmatics,
not local embeddings.

References:
- https://docs.speechmatics.com/speech-to-text/batch/speaker-identification
- https://docs.speechmatics.com/speech-to-text/features/speaker-identification
"""

import json
import os
import time
import tempfile
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any

try:
    import requests
except ImportError:
    requests = None

from .base import EmbeddingBackend


# API version - update when Speechmatics releases new API versions
API_VERSION = "v2"

# API regions
REGIONS = {
    'eu': f'https://eu1.asr.api.speechmatics.com/{API_VERSION}',
    'eu1': f'https://eu1.asr.api.speechmatics.com/{API_VERSION}',
    'us': f'https://us1.asr.api.speechmatics.com/{API_VERSION}',
    'us1': f'https://us1.asr.api.speechmatics.com/{API_VERSION}',
    'au': f'https://au1.asr.api.speechmatics.com/{API_VERSION}',
    'au1': f'https://au1.asr.api.speechmatics.com/{API_VERSION}',
}

# Compatible model versions for identification
# Embeddings created with older versions may not work with newer API
COMPATIBLE_MODEL_VERSIONS = {
    "speechmatics-v2",  # Current version
}


class SpeechmaticsBackend(EmbeddingBackend):
    """
    Speechmatics API-based speaker identification backend.

    Uses Speechmatics batch API to generate encrypted speaker identifiers
    from audio samples. Identifiers are opaque strings that can only be
    used with the Speechmatics API.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        region: str = "eu",
        language: str = "en",
    ):
        """
        Initialize Speechmatics backend.

        Args:
            api_key: Speechmatics API key (defaults to SPEECHMATICS_API_KEY env)
            region: API region (eu, us, au)
            language: Default language for transcription
        """
        if requests is None:
            raise ImportError("requests library required: pip install requests")

        self._api_key = api_key or os.environ.get("SPEECHMATICS_API_KEY")
        if not self._api_key:
            raise ValueError(
                "Speechmatics API key required. "
                "Set SPEECHMATICS_API_KEY or pass api_key parameter."
            )

        self._region = region
        self._base_url = REGIONS.get(region, REGIONS["eu"])
        self._language = language

    @property
    def name(self) -> str:
        return "speechmatics"

    @property
    def requires_api_key(self) -> bool:
        return True

    @property
    def audio_profile(self) -> str:
        """Return the speechmatics audio profile name."""
        return "speechmatics"

    @property
    def api_version(self) -> str:
        """Current API version string."""
        return API_VERSION

    @property
    def model_version(self) -> str:
        """Model version string stored in embeddings."""
        return f"speechmatics-{API_VERSION}"

    def check_embedding_compatibility(
        self,
        embedding: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Check if an embedding is compatible with current API version.

        Args:
            embedding: Embedding dict with model_version field

        Returns:
            Dict with:
            - compatible: bool
            - version: embedding's model_version
            - current: current model_version
            - warning: warning message if incompatible
        """
        emb_version = embedding.get("model_version", "unknown")
        result = {
            "compatible": emb_version in COMPATIBLE_MODEL_VERSIONS,
            "version": emb_version,
            "current": self.model_version,
            "warning": None,
        }
        if not result["compatible"]:
            result["warning"] = (
                f"Embedding created with {emb_version} may not work with "
                f"current API {self.model_version}. Consider re-enrolling."
            )
        return result

    def _headers(self) -> Dict[str, str]:
        """Get API request headers."""
        return {"Authorization": f"Bearer {self._api_key}"}

    def _create_job(
        self,
        audio_path: Path,
        config: Dict[str, Any],
    ) -> str:
        """
        Submit a transcription job.

        Args:
            audio_path: Path to audio file
            config: Job configuration dict

        Returns:
            Job ID
        """
        url = f"{self._base_url}/jobs"

        with open(audio_path, "rb") as f:
            files = {"data_file": (audio_path.name, f)}
            data = {"config": json.dumps(config)}
            response = requests.post(
                url, headers=self._headers(), files=files, data=data
            )

        response.raise_for_status()
        return response.json()["id"]

    def _wait_for_job(
        self,
        job_id: str,
        poll_interval: float = 3.0,
        max_wait: float = 600.0,
    ) -> Dict[str, Any]:
        """
        Wait for job completion.

        Args:
            job_id: Job ID to monitor
            poll_interval: Seconds between status checks
            max_wait: Maximum wait time in seconds

        Returns:
            Final job status dict

        Raises:
            TimeoutError: If job doesn't complete in time
            Exception: If job is rejected
        """
        url = f"{self._base_url}/jobs/{job_id}"
        start_time = time.time()

        while True:
            elapsed = time.time() - start_time
            if elapsed > max_wait:
                raise TimeoutError(f"Job {job_id} did not complete within {max_wait}s")

            response = requests.get(url, headers=self._headers())
            response.raise_for_status()
            job = response.json()["job"]
            status = job.get("status")

            if status == "done":
                return job
            elif status == "rejected":
                error_msg = job.get("errors", [{}])[0].get("message", "Unknown error")
                raise Exception(f"Job rejected: {error_msg}")
            elif status in ("running", "pending"):
                time.sleep(poll_interval)
            else:
                time.sleep(poll_interval)

    def _get_transcript(self, job_id: str) -> Dict[str, Any]:
        """
        Get transcript JSON for completed job.

        Args:
            job_id: Completed job ID

        Returns:
            Transcript JSON dict
        """
        url = f"{self._base_url}/jobs/{job_id}/transcript"
        params = {"format": "json-v2"}

        response = requests.get(url, headers=self._headers(), params=params)
        response.raise_for_status()
        return response.json()

    def _extract_audio_segment(
        self,
        audio_path: Path,
        segments: List[Tuple[float, float]],
    ) -> Path:
        """
        Extract audio segments to a temporary file.

        Args:
            audio_path: Source audio file
            segments: List of (start_sec, end_sec) tuples

        Returns:
            Path to temporary WAV file with extracted segments
        """
        try:
            import subprocess
        except ImportError:
            raise ImportError("subprocess required for audio extraction")

        # Build ffmpeg filter for multiple segments
        # Using concat filter to join segments
        filter_parts = []
        for i, (start, end) in enumerate(segments):
            duration = end - start
            filter_parts.append(f"[0:a]atrim=start={start}:duration={duration},asetpts=PTS-STARTPTS[a{i}]")

        if len(segments) == 1:
            filter_complex = filter_parts[0].replace(f"[a0]", "")
            output_map = None
        else:
            concat_inputs = "".join(f"[a{i}]" for i in range(len(segments)))
            filter_complex = ";".join(filter_parts) + f";{concat_inputs}concat=n={len(segments)}:v=0:a=1"
            output_map = None

        # Create temp file
        temp_fd, temp_path = tempfile.mkstemp(suffix=".wav")
        os.close(temp_fd)

        try:
            cmd = [
                "ffmpeg", "-y", "-i", str(audio_path),
                "-filter_complex", filter_complex,
                "-ac", "1", "-ar", "16000",
                temp_path
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            return Path(temp_path)
        except subprocess.CalledProcessError as e:
            os.unlink(temp_path)
            raise RuntimeError(f"ffmpeg extraction failed: {e.stderr.decode()}")

    def enroll_speaker(
        self,
        audio_path: Path,
        segments: Optional[List[Tuple[float, float]]] = None,
    ) -> Dict[str, Any]:
        """
        Enroll a speaker from audio file.

        Runs a diarization job with get_speakers=true to extract
        speaker identifiers from the audio.

        Args:
            audio_path: Path to audio file (5-30s of target speaker)
            segments: Optional time segments to extract first

        Returns:
            Dict with:
            - external_id: Speechmatics speaker identifier string
            - model_version: API version
            - source_audio: Original audio path
            - source_segments: Segments used (if any)
        """
        audio_path = Path(audio_path)
        temp_file = None

        try:
            # Extract segments if specified
            if segments:
                temp_file = self._extract_audio_segment(audio_path, segments)
                work_audio = temp_file
            else:
                work_audio = audio_path

            # Create enrollment job with get_speakers enabled
            config = {
                "type": "transcription",
                "transcription_config": {
                    "language": self._language,
                    "diarization": "speaker",
                    "speaker_diarization_config": {
                        "get_speakers": True,
                    },
                },
            }

            job_id = self._create_job(work_audio, config)
            self._wait_for_job(job_id)
            transcript = self._get_transcript(job_id)

            # Extract speaker identifiers from response
            speakers = transcript.get("speakers", [])
            if not speakers:
                raise ValueError(
                    "No speakers detected in audio. "
                    "Ensure audio contains clear speech from the target speaker."
                )

            # Use the first speaker's identifiers (assuming single-speaker enrollment)
            # If multiple speakers, take the one with most identifiers
            best_speaker = max(speakers, key=lambda s: len(s.get("speaker_identifiers", [])))
            identifiers = best_speaker.get("speaker_identifiers", [])

            if not identifiers:
                raise ValueError("No speaker identifiers generated. Audio may be too short.")

            return {
                "external_id": identifiers[0],  # Primary identifier
                "all_identifiers": identifiers,  # All generated identifiers
                "model_version": f"speechmatics-{API_VERSION}",
                "source_audio": str(audio_path),
                "source_segments": segments,
                "region": self._region,
            }

        finally:
            if temp_file and temp_file.exists():
                temp_file.unlink()

    def identify_speaker(
        self,
        audio_path: Path,
        candidates: List[Dict[str, Any]],
        threshold: float = 0.354,
    ) -> List[Dict[str, Any]]:
        """
        Identify speaker in audio from candidate embeddings.

        Runs a diarization job with speaker identification enabled,
        matching against enrolled speaker identifiers.

        Args:
            audio_path: Path to audio file to identify
            candidates: List of speaker profiles with embeddings
            threshold: Not used (Speechmatics handles internally)

        Returns:
            List of matches with:
            - speaker_id: Matched speaker ID
            - label: Label used in transcription
            - confidence: Always 1.0 (Speechmatics binary match)
        """
        audio_path = Path(audio_path)

        # Build speakers config from candidates
        speakers_config = []
        id_to_speaker = {}

        for candidate in candidates:
            speaker_id = candidate.get("id")
            embeddings = candidate.get("embeddings", {}).get("speechmatics", [])

            if not embeddings:
                continue

            # Check compatibility and warn if needed
            for emb in embeddings:
                compat = self.check_embedding_compatibility(emb)
                if not compat["compatible"]:
                    import sys
                    print(
                        f"Warning: {speaker_id}: {compat['warning']}",
                        file=sys.stderr,
                    )
                    break  # Only warn once per speaker

            if not embeddings:
                continue

            # Collect all identifiers for this speaker
            all_identifiers = []
            for emb in embeddings:
                ext_id = emb.get("external_id")
                if ext_id:
                    all_identifiers.append(ext_id)
                # Also include any additional identifiers
                all_identifiers.extend(emb.get("all_identifiers", []))

            if all_identifiers:
                # Use speaker ID as label
                label = speaker_id
                speakers_config.append({
                    "label": label,
                    "speaker_identifiers": list(set(all_identifiers))[:50],  # Max 50 per API
                })
                id_to_speaker[label] = speaker_id

        if not speakers_config:
            return []

        # Create identification job
        config = {
            "type": "transcription",
            "transcription_config": {
                "language": self._language,
                "diarization": "speaker",
                "speaker_diarization_config": {
                    "speakers": speakers_config,
                },
            },
        }

        job_id = self._create_job(audio_path, config)
        self._wait_for_job(job_id)
        transcript = self._get_transcript(job_id)

        # Extract identified speakers from results
        results = transcript.get("results", [])
        identified = set()
        all_speakers_found = set()  # For debugging

        for item in results:
            if item.get("type") != "word":
                continue
            # Speaker label can be at top level or inside alternatives
            speaker = item.get("speaker")
            if not speaker:
                # Check inside alternatives array (Speechmatics format)
                alternatives = item.get("alternatives", [])
                if alternatives:
                    speaker = alternatives[0].get("speaker")
            if speaker:
                all_speakers_found.add(speaker)
                if speaker in id_to_speaker:
                    identified.add(speaker)

        # Debug: log what speakers were found vs expected
        import os
        if os.environ.get("SPEAKER_DETECTION_DEBUG"):
            import sys
            print(f"DEBUG: speakers_config labels: {[s['label'] for s in speakers_config]}", file=sys.stderr)
            print(f"DEBUG: all speakers found in transcript: {all_speakers_found}", file=sys.stderr)
            print(f"DEBUG: matched to candidates: {identified}", file=sys.stderr)
            print(f"DEBUG: transcript keys: {transcript.keys()}", file=sys.stderr)
            print(f"DEBUG: results count: {len(results)}", file=sys.stderr)
            if results:
                print(f"DEBUG: first result: {results[0]}", file=sys.stderr)

        # Return matches
        matches = []
        for label in identified:
            matches.append({
                "speaker_id": id_to_speaker[label],
                "label": label,
                "confidence": 1.0,  # Speechmatics doesn't provide confidence scores
            })

        return matches

    def verify_speaker(
        self,
        audio_path: Path,
        speaker_profile: Dict[str, Any],
        threshold: float = 0.354,
    ) -> Dict[str, Any]:
        """
        Verify if audio matches a specific speaker.

        Args:
            audio_path: Path to audio file
            speaker_profile: Speaker profile with embeddings
            threshold: Not used (Speechmatics handles internally)

        Returns:
            Dict with:
            - match: bool
            - confidence: 1.0 if match, 0.0 otherwise
            - embedding_id: Which embedding matched (if any)
        """
        results = self.identify_speaker(audio_path, [speaker_profile], threshold)
        if results:
            return {
                "match": True,
                "confidence": 1.0,
                "embedding_id": results[0].get("label"),
            }
        return {"match": False, "confidence": 0.0, "embedding_id": None}


# Module-level Backend class for registry
Backend = SpeechmaticsBackend
